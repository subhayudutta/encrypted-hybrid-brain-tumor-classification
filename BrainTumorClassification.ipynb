{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Importing Essential Libraries***"
      ],
      "metadata": {
        "id": "E_M2z2a_hXND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsOcT-WJhSer"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import skew, kurtosis\n",
        "import os\n",
        "from skimage.feature.texture import graycomatrix, graycoprops, local_binary_pattern\n",
        "import pywt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Feature Extraction***"
      ],
      "metadata": {
        "id": "B7ifF2jGhnYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phi = 1.618  # golden ratio\n",
        "\n",
        "# --- Divide a line segment using the golden ratio ---\n",
        "def divide_point(p1, p2, ratio=phi):\n",
        "    x = int((p1[0] + ratio * p2[0]) / (1 + ratio))\n",
        "    y = int((p1[1] + ratio * p2[1]) / (1 + ratio))\n",
        "    return (x, y)\n",
        "\n",
        "# --- Preprocessing: create a square-padded version of the image ---\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        return None\n",
        "    h, w, _ = img.shape\n",
        "    side = max(h, w)\n",
        "    square_img = np.full((side, side, 3), (0, 0, 0), dtype=np.uint8)\n",
        "    x_offset = (side - w) // 2\n",
        "    y_offset = (side - h) // 2\n",
        "    square_img[y_offset:y_offset+h, x_offset:x_offset+w] = img\n",
        "    return square_img\n",
        "\n",
        "# --- Compute statistics from a triangular region ---\n",
        "def extract_triangle_features(img, pts):\n",
        "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "    cv2.fillPoly(mask, [np.array(pts)], 255)\n",
        "    block = cv2.bitwise_and(img, img, mask=mask)\n",
        "    gray = cv2.cvtColor(block, cv2.COLOR_BGR2GRAY)\n",
        "    pixels = gray[gray > 0]\n",
        "    if len(pixels) == 0:\n",
        "        return [0, 0, 0, 0, 0]\n",
        "    return [np.mean(pixels), np.median(pixels), np.std(pixels), skew(pixels), kurtosis(pixels)]\n",
        "\n",
        "# --- Perform golden ratio subdivision: return 4 sub-triangles----\n",
        "def golden_subdivide(A, B, C):\n",
        "    D = divide_point(A, C, phi)\n",
        "    E = divide_point(B, D, phi)\n",
        "    F = divide_point(C, E, phi)\n",
        "    return {\n",
        "        \"ABD\": [A, B, D],\n",
        "        \"BCE\": [B, C, E],\n",
        "        \"DFC\": [D, F, C],\n",
        "        \"DEF\": [D, E, F]\n",
        "    }\n",
        "\n",
        "# --- Construct the initial 6 triangles forming a hexagon around a circle ---\n",
        "def split_into_triangles(img):\n",
        "    h, w, _ = img.shape\n",
        "    cx, cy = w // 2, h // 2\n",
        "    radius = int(np.sqrt((w/2)**2 + (h/2)**2))\n",
        "    hexagon = []\n",
        "    for i in range(6):\n",
        "        angle = np.deg2rad(60 * i - 30)\n",
        "        x = int(cx + radius * np.cos(angle))\n",
        "        y = int(cy + radius * np.sin(angle))\n",
        "        hexagon.append((x, y))\n",
        "    triangles = []\n",
        "    for i in range(6):\n",
        "        pt1, pt2, pt3 = (cx, cy), hexagon[i], hexagon[(i+1) % 6]\n",
        "        triangles.append([pt1, pt2, pt3])\n",
        "    return triangles, (cx, cy), radius, hexagon\n",
        "\n",
        "# --- Box-counting fractal dimension ---\n",
        "def fractal_dimension(Z, threshold=0.9):\n",
        "    Z = (Z < threshold * Z.max())\n",
        "    sizes = 2 ** np.arange(1, int(np.log2(min(Z.shape))) )\n",
        "    counts = []\n",
        "    for size in sizes:\n",
        "        S = np.add.reduceat(\n",
        "            np.add.reduceat(Z, np.arange(0, Z.shape[0], size), axis=0),\n",
        "            np.arange(0, Z.shape[1], size), axis=1)\n",
        "        counts.append(np.sum(S > 0))\n",
        "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
        "    return -coeffs[0]\n",
        "\n",
        "\n",
        "# --- Lacunarity ---\n",
        "def lacunarity(Z, box_sizes=[2,4,8]):\n",
        "    Z = (Z > 0).astype(np.uint8)\n",
        "    lac = []\n",
        "    for size in box_sizes:\n",
        "        S = np.add.reduceat(\n",
        "            np.add.reduceat(Z, np.arange(0, Z.shape[0], size), axis=0),\n",
        "            np.arange(0, Z.shape[1], size), axis=1)\n",
        "        mean = np.mean(S)\n",
        "        var = np.var(S)\n",
        "        if mean > 0:\n",
        "            lac.append(var / (mean**2))\n",
        "        else:\n",
        "            lac.append(0)\n",
        "    return lac\n",
        "\n",
        "\n",
        "# --- Wavelet Energy ---\n",
        "def wavelet_energy(Z, wavelet='db2', level=2):\n",
        "    coeffs = pywt.wavedec2(Z, wavelet, level=level)\n",
        "    energies = []\n",
        "    for c in coeffs[1:]:\n",
        "        for band in c:\n",
        "            energies.append(np.sum(np.square(band)))\n",
        "    return energies\n",
        "\n",
        "def process_image(img_path, label):\n",
        "    img = preprocess_image(img_path)\n",
        "    if img is None:\n",
        "        return None\n",
        "\n",
        "    triangles, _, _, _ = split_into_triangles(img)\n",
        "    feats_img = {}\n",
        "\n",
        "    for i, tri in enumerate(triangles):\n",
        "        subtris = golden_subdivide(tri[1], tri[2], tri[0])\n",
        "        for j, (name, subtri) in enumerate(subtris.items(), start=1):\n",
        "\n",
        "            # ---- Basic Stats ---- #\n",
        "            feats = extract_triangle_features(img, subtri)\n",
        "            feats_img[f\"T{i+1}_{name}_mean\"] = feats[0]\n",
        "            feats_img[f\"T{i+1}_{name}_median\"] = feats[1]\n",
        "            feats_img[f\"T{i+1}_{name}_std\"] = feats[2]\n",
        "            feats_img[f\"T{i+1}_{name}_skew\"] = feats[3]\n",
        "            feats_img[f\"T{i+1}_{name}_kurt\"] = feats[4]\n",
        "\n",
        "            # ---- GLCM Features ---- #\n",
        "            mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
        "            pts = np.array([subtri], np.int32)\n",
        "            cv2.fillPoly(mask, [pts], 255)\n",
        "\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            block = cv2.bitwise_and(gray, gray, mask=mask)\n",
        "\n",
        "            pixels = block[mask == 255]\n",
        "\n",
        "            if pixels.size > 0:\n",
        "                block_u8 = np.uint8(pixels.reshape(-1, 1))\n",
        "                glcm = graycomatrix(block_u8,\n",
        "                            distances=[1],\n",
        "                            angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                            symmetric=True, normed=True)\n",
        "\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_contrast\"] = graycoprops(glcm, 'contrast').mean()\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_correlation\"] = graycoprops(glcm, 'correlation').mean()\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_energy\"] = graycoprops(glcm, 'energy').mean()\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_homogeneity\"] = graycoprops(glcm, 'homogeneity').mean()\n",
        "            else:\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_contrast\"] = 0\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_correlation\"] = 0\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_energy\"] = 0\n",
        "                feats_img[f\"T{i+1}_{name}_glcm_homogeneity\"] = 0\n",
        "\n",
        "            lbp = local_binary_pattern(block, P=8, R=1, method=\"uniform\")\n",
        "            lbp_mean = lbp.mean()\n",
        "            lbp_hist, _ = np.histogram(lbp.ravel(),\n",
        "                                       bins=np.arange(0, 8+3),\n",
        "                                       range=(0, 8+2),\n",
        "                                       density=True)\n",
        "\n",
        "            feats_img[f\"T{i+1}_{name}_lbp_mean\"] = lbp_mean\n",
        "            for k, v in enumerate(lbp_hist):\n",
        "                feats_img[f\"T{i+1}_{name}_lbp_hist_{k}\"] = v\n",
        "\n",
        "            # ---- Edge Density ---- #\n",
        "            edges = cv2.Canny(block, 100, 200)\n",
        "            edge_density = np.sum(edges > 0) / float(block.size)\n",
        "            feats_img[f\"T{i+1}_{name}_edge_density\"] = edge_density\n",
        "\n",
        "            # ---- Gradient Stats ---- #\n",
        "            grad_x = cv2.Sobel(block, cv2.CV_64F, 1, 0, ksize=3)\n",
        "            grad_y = cv2.Sobel(block, cv2.CV_64F, 0, 1, ksize=3)\n",
        "            grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
        "\n",
        "            feats_img[f\"T{i+1}_{name}_gradient_mean\"] = grad_mag.mean()\n",
        "            feats_img[f\"T{i+1}_{name}_gradient_std\"] = grad_mag.std()\n",
        "\n",
        "            # ---- Fractal Dimension ---- #\n",
        "            try:\n",
        "                fd = fractal_dimension(block)\n",
        "            except:\n",
        "                fd = 0\n",
        "            feats_img[f\"T{i+1}_{name}_fractal_dim\"] = fd\n",
        "\n",
        "            # ---- Lacunarity ---- #\n",
        "            lac_vals = lacunarity(block, box_sizes=[2,4,8])\n",
        "            for k, v in enumerate(lac_vals):\n",
        "                feats_img[f\"T{i+1}_{name}_lacunarity_r{[2,4,8][k]}\"] = v\n",
        "\n",
        "            # ---- Wavelet Energy ---- #\n",
        "            wavelet_vals = wavelet_energy(block, wavelet='db2', level=2)\n",
        "            for k, v in enumerate(wavelet_vals):\n",
        "                feats_img[f\"T{i+1}_{name}_wavelet_energy_{k}\"] = v\n",
        "\n",
        "    feats_img[\"label\"] = label\n",
        "    return feats_img\n",
        "\n",
        "\n",
        "# --- Creating a CSV Dataset ---\n",
        "def process_dataset(dataset_path, save_csv=True):\n",
        "    label_map = {\"notumor\":0, \"glioma\":1, \"meningioma\":2, \"pituitary\":3}\n",
        "    all_features = []\n",
        "    for label in label_map.keys():\n",
        "        folder = os.path.join(dataset_path, label)\n",
        "        if not os.path.exists(folder):\n",
        "            continue\n",
        "        for file in os.listdir(folder):\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".jpeg\")):\n",
        "                img_path = os.path.join(folder, file)\n",
        "                feats = process_image(img_path, label_map[label])\n",
        "                if feats is not None:\n",
        "                    all_features.append(feats)\n",
        "    df = pd.DataFrame(all_features)\n",
        "    if save_csv:\n",
        "        df.to_csv(\"dataset_features_golden.csv\", index=False)\n",
        "    return df\n",
        "\n",
        "# --- Code to visualise the whirling triangle in image ---\n",
        "def visualize_shapes(img_path):\n",
        "    img = preprocess_image(img_path)\n",
        "    h, w, _ = img.shape\n",
        "    radius = int(np.sqrt((w/2)**2 + (h/2)**2))\n",
        "    canvas_size = 2 * radius + 50\n",
        "    canvas = np.full((canvas_size, canvas_size, 3), (50, 50, 50), dtype=np.uint8)\n",
        "    x_offset = (canvas_size - w) // 2\n",
        "    y_offset = (canvas_size - h) // 2\n",
        "    canvas[y_offset:y_offset+h, x_offset:x_offset+w] = img\n",
        "    cx, cy = canvas_size // 2, canvas_size // 2\n",
        "\n",
        "    cv2.circle(canvas, (cx, cy), radius, (0, 255, 0), 3)\n",
        "    hexagon = []\n",
        "    for i in range(6):\n",
        "        angle = np.deg2rad(60 * i - 30)\n",
        "        x = int(cx + radius * np.cos(angle))\n",
        "        y = int(cy + radius * np.sin(angle))\n",
        "        hexagon.append((x, y))\n",
        "    cv2.polylines(canvas, [np.array(hexagon, np.int32)], isClosed=True, color=(0, 0, 255), thickness=3)\n",
        "\n",
        "    colors = [(255,255,0),(0,255,255),(255,0,255),(0,128,255)]\n",
        "    for i in range(6):\n",
        "        pt1, pt2, pt3 = (cx, cy), hexagon[i], hexagon[(i+1) % 6]\n",
        "        cv2.polylines(canvas, [np.array([pt1, pt2, pt3], np.int32)], True, (255, 255, 255), 3)\n",
        "        subtris = golden_subdivide(pt2, pt3, pt1)\n",
        "        for j, subtri in enumerate(subtris.values()):\n",
        "            cv2.polylines(canvas, [np.array(subtri, np.int32)], True, colors[j % len(colors)], 3)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "dataset_path = \"E:/Research Papers/New folder/Codes/Brain Tumour Dataset\"\n"
      ],
      "metadata": {
        "id": "CA6yXLx5htc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = process_dataset(dataset_path, save_csv=True)\n",
        "print(\"CSV saved: braintumor_features.csv\")"
      ],
      "metadata": {
        "id": "R8lRy9kzjIMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Visualising the different features in the image***"
      ],
      "metadata": {
        "id": "z4pBZt-EjM2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_glcm_contrast(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    glcm = graycomatrix(img, distances=[1], angles=[0], symmetric=True, normed=True)\n",
        "    contrast = graycoprops(glcm, 'contrast')\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(contrast, cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"GLCM Contrast Map\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_lbp(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    lbp = local_binary_pattern(img, P=8, R=1, method='uniform')\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(lbp, cmap='gray')\n",
        "    plt.title(\"Local Binary Pattern (LBP)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_edges(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    edges = cv2.Canny(img, 100, 200)\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(edges, cmap='gray')\n",
        "    plt.title(\"Edge Map (Canny)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_fractal_lacunarity(img_path, threshold=128):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    Z = (img > threshold).astype(int)\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(Z, cmap='gray')\n",
        "    plt.title(\"Binary Tumor Block (Fractal / Lacunarity)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_wavelet_energy(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    coeffs2 = pywt.wavedec2(img, 'db2', level=2)\n",
        "    cA2, (cH2, cV2, cD2), (cH1, cV1, cD1) = coeffs2\n",
        "\n",
        "    energy_map = np.square(cH2) + np.square(cV2) + np.square(cD2)\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(energy_map, cmap='inferno')\n",
        "    plt.title(\"Wavelet Energy Map (Level 2)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def visualize_gradient(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    gx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(grad_mag, cmap='inferno')\n",
        "    plt.title(\"Sobel Gradient Magnitude\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hal2dQY6jT5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = \"Brain Tumour Dataset/Te-me_0010.jpg\"\n",
        "\n",
        "visualize_shapes(img_path)\n",
        "visualize_lbp(img_path)\n",
        "visualize_edges(img_path)\n",
        "visualize_gradient(img_path)\n",
        "visualize_glcm_contrast(img_path)\n",
        "visualize_fractal_lacunarity(img_path)\n",
        "visualize_wavelet_energy(img_path"
      ],
      "metadata": {
        "id": "y80CVmotjVEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Preprocessing The Dataset And Applying PCA***"
      ],
      "metadata": {
        "id": "6sXBK_C5j40p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "\n",
        "X = df.drop(\"label\", axis=1).values\n",
        "y = df[\"label\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)"
      ],
      "metadata": {
        "id": "uZdcF3ZOkACa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Training The Machine Learning Models Using The Generated Dataset***"
      ],
      "metadata": {
        "id": "c4MN5TK8jbQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from Pyfhel import Pyfhel, PyCtxt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "uJ0sZs0ejsYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM (RBF)\": SVC(kernel='rbf', probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', use_label_encoder=False),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"CatBoost\": CatBoostClassifier(verbose=0)\n",
        "}\n",
        "\n",
        "# --- Initialize Pyfhel for Homomorphic Encryption ---\n",
        "HE = Pyfhel()\n",
        "HE.contextGen(p=65537, m=4096, base=2)\n",
        "HE.keyGen()\n",
        "\n",
        "def encrypt_matrix(X):\n",
        "    \"\"\"Encrypt a 2D numpy array feature-wise.\"\"\"\n",
        "    X_enc = []\n",
        "    for row in X:\n",
        "        X_enc.append([HE.encryptFrac(val) for val in row])\n",
        "    return X_enc\n",
        "\n",
        "def decrypt_matrix(X_enc):\n",
        "    \"\"\"Decrypt a 2D encrypted matrix.\"\"\"\n",
        "    X_dec = []\n",
        "    for row in X_enc:\n",
        "        X_dec.append([HE.decryptFrac(val) for val in row])\n",
        "    return np.array(X_dec)\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # --- Plaintext ---\n",
        "    t0 = time.time()\n",
        "    model.fit(X_train_pca, y_train)\n",
        "    train_time = time.time() - t0\n",
        "\n",
        "    t1 = time.time()\n",
        "    y_pred = model.predict(X_test_pca)\n",
        "    test_time = time.time() - t1\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    prec = precision_score(y_test, y_pred, average='weighted')\n",
        "    rec = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Data\": \"Plaintext\",\n",
        "        \"Accuracy\": round(acc,4),\n",
        "        \"F1\": round(f1,4),\n",
        "        \"Precision\": round(prec,4),\n",
        "        \"Recall\": round(rec,4),\n",
        "        \"TrainTime(s)\": round(train_time,2),\n",
        "        \"TestTime(s)\": round(test_time,2)\n",
        "    })\n",
        "\n",
        "    if name == \"Logistic Regression\":\n",
        "        X_train_enc = encrypt_matrix(X_train_pca)\n",
        "        X_test_enc = encrypt_matrix(X_test_pca)\n",
        "\n",
        "        X_train_dec = decrypt_matrix(X_train_enc)\n",
        "        X_test_dec = decrypt_matrix(X_test_enc)\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.fit(X_train_dec, y_train)\n",
        "        train_time = time.time() - t0\n",
        "\n",
        "        t1 = time.time()\n",
        "        y_pred = model.predict(X_test_dec)\n",
        "        test_time = time.time() - t1\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        prec = precision_score(y_test, y_pred, average='weighted')\n",
        "        rec = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": name,\n",
        "            \"Data\": \"Encrypted\",\n",
        "            \"Accuracy\": round(acc,4),\n",
        "            \"F1\": round(f1,4),\n",
        "            \"Precision\": round(prec,4),\n",
        "            \"Recall\": round(rec,4),\n",
        "            \"TrainTime(s)\": round(train_time,2),\n",
        "            \"TestTime(s)\": round(test_time,2)\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "ofhIBqogjxBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Training The Transfer Learning Models Using The Image Dataset***"
      ],
      "metadata": {
        "id": "nQT6PEiCkoCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, DenseNet121, MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "b30KplOgks8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (224, 224)\n",
        "batch_size = 16\n",
        "epochs = 100\n",
        "data_dir = \"/content/drive/MyDrive/Brain Tumor Dataset\"\n",
        "classes = sorted(os.listdir(data_dir))\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15\n",
        ")\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\",\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "num_classes = len(train_gen.class_indices)"
      ],
      "metadata": {
        "id": "8_jm7R3TlH7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TL models to compare ---\n",
        "tl_models = {\n",
        "    \"VGG16\": VGG16,\n",
        "    \"VGG19\": VGG19,\n",
        "    \"ResNet50\": ResNet50,\n",
        "    \"InceptionV3\": InceptionV3,\n",
        "    \"DenseNet121\": DenseNet121,\n",
        "    \"MobileNetV2\": MobileNetV2\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, base_model_class in tl_models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    # Loading base model without top\n",
        "    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Freeze base layers initially\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Compile\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    val_gen.reset()\n",
        "    y_true = val_gen.classes\n",
        "    y_pred_probs = model.predict(val_gen, verbose=0)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    prec = precision_score(y_true, y_pred, average='weighted')\n",
        "    rec = recall_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": round(acc,4),\n",
        "        \"F1\": round(f1,4),\n",
        "        \"Precision\": round(prec,4),\n",
        "        \"Recall\": round(rec,4)\n",
        "    })\n",
        "\n",
        "# --- Save results ---\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nTransfer Learning Models Comparison:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "ruRsyazhlD24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implementing Hybrid Model (Image+text)***"
      ],
      "metadata": {
        "id": "8N_7OpZLlNVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ],
      "metadata": {
        "id": "PmrBTquRlR8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = \"/content/drive/MyDrive/Brain Tumor Dataset/Testing\"\n",
        "CSV_FILE = \"/content/dataset_features_golden.csv\"\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "RANDOM_SEED = 42\n",
        "PCA_COMPONENTS = 112\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "df = pd.read_csv(CSV_FILE)\n",
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "X_csv = df.values.astype(np.float32)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_csv_scaled = scaler.fit_transform(X_csv)\n",
        "\n",
        "if X_csv_scaled.shape[1] > PCA_COMPONENTS:\n",
        "    pca = PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_SEED)\n",
        "    X_csv_pca = pca.fit_transform(X_csv_scaled)\n",
        "else:\n",
        "    X_csv_pca = X_csv_scaled\n",
        "\n",
        "print(\"CSV features shape:\", X_csv_pca.shape)\n",
        "\n",
        "class_names = sorted(os.listdir(DATASET_DIR))\n",
        "num_classes = len(class_names)\n",
        "\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for label, cls in enumerate(class_names):\n",
        "    cls_folder = os.path.join(DATASET_DIR, cls)\n",
        "    for fname in sorted(os.listdir(cls_folder)):\n",
        "        fpath = os.path.join(cls_folder, fname)\n",
        "        image_paths.append(fpath)\n",
        "        labels.append(label)\n",
        "\n",
        "print(\"Total images found:\", len(image_paths))\n",
        "print(\"Total CSV rows:\", X_csv_pca.shape[0])\n",
        "assert len(image_paths) == X_csv_pca.shape[0], \"Mismatch between images and CSV rows!\"\n",
        "\n",
        "def load_and_preprocess(path):\n",
        "    img = load_img(path, target_size=IMG_SIZE)\n",
        "    arr = img_to_array(img).astype(\"float32\") / 255.0\n",
        "    return arr\n",
        "\n",
        "images = np.array([load_and_preprocess(p) for p in image_paths])\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Images shape:\", images.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "X_img_train, X_img_test, X_csv_train, X_csv_test, y_train, y_test = train_test_split(\n",
        "    images, X_csv_pca, labels, test_size=0.2, random_state=RANDOM_SEED, stratify=labels\n",
        ")"
      ],
      "metadata": {
        "id": "ZU587EJ2lhjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_image_branch(input_shape=IMG_SIZE+(3,), embedding_dim=128):\n",
        "    inp = layers.Input(shape=input_shape, name='img_input')\n",
        "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inp)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(embedding_dim, activation='relu')(x)\n",
        "    return models.Model(inp, x, name='image_branch')\n",
        "\n",
        "def build_csv_branch(input_dim, embedding_dim=32):\n",
        "    inp = layers.Input(shape=(input_dim,), name='csv_input')\n",
        "    x = layers.Dense(128, activation='relu')(inp)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dense(embedding_dim, activation='relu')(x)\n",
        "    return models.Model(inp, x, name='csv_branch')\n",
        "\n",
        "def build_hybrid_model(img_shape, csv_dim, num_classes):\n",
        "    img_branch = build_image_branch(input_shape=img_shape, embedding_dim=128)\n",
        "    csv_branch = build_csv_branch(input_dim=csv_dim, embedding_dim=32)\n",
        "\n",
        "    fused = layers.Concatenate()([img_branch.output, csv_branch.output])\n",
        "    x = layers.Dense(128, activation='relu')(fused)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=[img_branch.input, csv_branch.input], outputs=out, name='hybrid_model')\n",
        "    return model\n",
        "\n",
        "model = build_hybrid_model(\n",
        "    img_shape=IMG_SIZE+(3,),\n",
        "    csv_dim=X_csv_pca.shape[1],\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(1e-4),\n",
        "    loss=losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "p-0sX7wSl6D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    [X_img_train, X_csv_train],\n",
        "    y_train,\n",
        "    validation_data=([X_img_test, X_csv_test], y_test),\n",
        "    epochs=1,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate([X_img_test, X_csv_test], y_test, verbose=0)\n",
        "print(\"Hybrid Model - Test Loss:\", test_loss, \"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "2dM6kn62l8XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implementing Homomorphic Encryption***"
      ],
      "metadata": {
        "id": "MxmkwZM3mGZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "X_csv = df.drop(\"label\", axis=1).values\n",
        "y_csv = df[\"label\"].values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_csv)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y_csv, test_size=0.2, random_state=42\n",
        ")\n",
        "print(f\"Original features shape: {X_csv.shape}\")\n",
        "print(f\"PCA features shape: {X_pca.shape}\")\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "logreg = LogisticRegression(max_iter=500, multi_class='ovr')\n",
        "logreg.fit(X_train, y_train)\n",
        "joblib.dump(logreg, \"logreg_csv_for_he.joblib\")\n",
        "acc = logreg.score(X_test, y_test)\n",
        "print(f\"Plain text accuracy: {acc:.4f}\")\n",
        "X_csv_test = X_test\n",
        "y_test = y_test\n",
        "\n",
        "# ---------- Prepare weights & bias ----------\n",
        "weights = logreg.coef_.astype(np.float64)\n",
        "bias = logreg.intercept_.astype(np.float64)\n",
        "\n",
        "n_classes, D = weights.shape\n",
        "print(f\"LogReg weights shape: {weights.shape}, bias shape: {bias.shape}\")\n",
        "\n",
        "# ---------- TenSEAL CKKS context (client) ----------\n",
        "# Use safe parameters: poly_modulus_degree must be power of two, >= 4096. 8192 is common.\n",
        "def make_tenseal_context(poly_modulus_degree=8192, coeff_mod_bit_sizes=[60, 40, 40, 60], global_scale=2**40):\n",
        "    ctx = ts.context(\n",
        "        ts.SCHEME_TYPE.CKKS,\n",
        "        poly_modulus_degree=poly_modulus_degree,\n",
        "        coeff_mod_bit_sizes=coeff_mod_bit_sizes\n",
        "    )\n",
        "    ctx.generate_galois_keys()\n",
        "    ctx.global_scale = global_scale\n",
        "    # IMPORTANT: in a real deployment, the client keeps the secret key and only shares public keys\n",
        "    return ctx\n",
        "\n",
        "ctx = make_tenseal_context()\n",
        "\n",
        "# ---------- Choose a small batch to demo ----------\n",
        "batch_size = min(20, X_csv_test.shape[0])\n",
        "batch_idx = np.arange(batch_size)\n",
        "X_batch = X_csv_test[batch_idx].astype(np.float64)\n",
        "y_batch = y_test[batch_idx] if 'y_test' in globals() else None\n",
        "\n",
        "# ---------- Encryption (client) ----------\n",
        "enc_start = time.time()\n",
        "enc_vectors = [ts.ckks_vector(ctx, X_batch[i].tolist()) for i in range(X_batch.shape[0])]\n",
        "enc_time = time.time() - enc_start\n",
        "print(f\"Encrypted {X_batch.shape[0]} vectors in {enc_time:.3f} s\")\n",
        "\n",
        "# ---------- Server-side computation (plaintext weights) ----------\n",
        "# For multiclass: compute encrypted logits for each class (one ciphertext per sample per class)\n",
        "server_start = time.time()\n",
        "enc_logits_per_sample = []  # list length B; each element will be list of ciphertext logits length C\n",
        "\n",
        "for enc_vec in enc_vectors:\n",
        "    enc_logits = []\n",
        "    # compute encrypted dot for each class weight\n",
        "    for c in range(n_classes):\n",
        "        w_c = weights[c].tolist()   # plaintext list\n",
        "        b_c = float(bias[c])\n",
        "        # elementwise multiplication (ciphertext * plaintext list)\n",
        "        enc_prod = enc_vec * w_c     # returns a CKKSVector ciphertext\n",
        "        # sum all slots to get scalar ciphertext (approx dot product)\n",
        "        enc_dot = enc_prod.sum()     # CKKSVector.sum() reduces to a single-slot ciphertext\n",
        "        enc_logit = enc_dot + b_c    # add plaintext bias\n",
        "        enc_logits.append(enc_logit)\n",
        "    enc_logits_per_sample.append(enc_logits)\n",
        "\n",
        "server_time = time.time() - server_start\n",
        "print(f\"Server computed encrypted logits in {server_time:.3f} s\")\n",
        "\n",
        "# ---------- Client-side decryption ----------\n",
        "dec_start = time.time()\n",
        "decoded_logits = np.zeros((X_batch.shape[0], n_classes), dtype=np.float64)\n",
        "for i in range(X_batch.shape[0]):\n",
        "    for c in range(n_classes):\n",
        "        val = enc_logits_per_sample[i][c].decrypt()[0]\n",
        "        decoded_logits[i, c] = float(val)\n",
        "dec_time = time.time() - dec_start\n",
        "print(f\"Decrypted logits for batch in {dec_time:.3f} s\")\n",
        "\n",
        "# ---------- Compute predictions & compare to plaintext ----------\n",
        "def softmax_rows(logit_matrix):\n",
        "    ex = np.exp(logit_matrix - np.max(logit_matrix, axis=1, keepdims=True))\n",
        "    return ex / (np.sum(ex, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "probs_he = softmax_rows(decoded_logits)\n",
        "pred_he = np.argmax(probs_he, axis=1)\n",
        "\n",
        "# Plaintext predictions for batch\n",
        "pred_plain = logreg.predict(X_batch)\n",
        "\n",
        "print(\"HE batch predictions:\", pred_he)\n",
        "print(\"Plain batch predictions:\", pred_plain)\n",
        "\n",
        "agree = np.mean(pred_he == pred_plain)\n",
        "print(f\"Agreement between HE predictions and plaintext logistic predictions: {agree*100:.2f}%\")\n",
        "\n",
        "# If true labels available, compute HE accuracy\n",
        "if y_batch is not None:\n",
        "    he_acc = accuracy_score(y_batch, pred_he)\n",
        "    plain_acc = accuracy_score(y_batch, pred_plain)\n",
        "    print(f\"HE accuracy on batch: {he_acc:.4f}, Plain accuracy on batch: {plain_acc:.4f}\")\n",
        "\n",
        "# ---------- Timings summary ----------\n",
        "print(f\"Timings (s) -> encrypt: {enc_time:.3f}, server: {server_time:.3f}, decrypt: {dec_time:.3f} for batch size {X_batch.shape[0]}\")\n"
      ],
      "metadata": {
        "id": "b666_kKWmJ2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Extracting LBP Mean and GLCM Distribution Across Dataset***"
      ],
      "metadata": {
        "id": "h2HuhGKqnGSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/braintumor_features.csv\")\n",
        "label_map = {0: \"No Tumor\", 1: \"Glioma\", 2: \"Meningioma\", 3: \"Pituitary\"}\n",
        "df['label_name'] = df['label'].map(label_map)\n",
        "\n",
        "selected_features = [\n",
        "    'T1_ABD_lbp_mean',\n",
        "    'T1_ABD_lbp_hist_0',\n",
        "    'T1_ABD_glcm_contrast',\n",
        "    'T1_ABD_glcm_energy',\n",
        "    'T1_ABD_edge_density',\n",
        "    'T1_ABD_gradient_mean'\n",
        "]\n",
        "\n",
        "summary_list = []\n",
        "for feature in selected_features:\n",
        "    row = {'Feature': feature}\n",
        "    for label_val, label_name in label_map.items():\n",
        "        mean = df[df['label'] == label_val][feature].mean()\n",
        "        std = df[df['label'] == label_val][feature].std()\n",
        "        row[label_name] = f\"{mean:.3f} Â± {std:.3f}\"\n",
        "    summary_list.append(row)\n",
        "\n",
        "summary_table = pd.DataFrame(summary_list)\n",
        "\n",
        "# LBP mean distribution across classes\n",
        "sns.boxplot(x='label_name', y='T1_ABD_lbp_mean', data=df)\n",
        "plt.title(\"LBP Mean Distribution Across Tumor Classes\")\n",
        "plt.xlabel(\"Tumor Class\")\n",
        "plt.ylabel(\"LBP Mean\")\n",
        "plt.show()\n",
        "\n",
        "# GLCM contrast distribution\n",
        "sns.boxplot(x='label_name', y='T1_ABD_glcm_contrast', data=df)\n",
        "plt.title(\"GLCM Contrast Distribution Across Tumor Classes\")\n",
        "plt.xlabel(\"Tumor Class\")\n",
        "plt.ylabel(\"GLCM Contrast\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "13Rjx89DnOYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z_XxouNdnbAv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRuLWFeYnmQf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}